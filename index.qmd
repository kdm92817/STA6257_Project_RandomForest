---
title: "STA 6257 - Data Science Capstone"
author: "Group Random Forest"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is random forest?

Random forests are considered an "out of the box" machine learning
algorithm and are made up of a collection of decision trees to create a
"forest" (Breiman 2001). This approach is useful in prediction and
regression analysis.

Random forests are constructed on the fundamental principles found
bagging, which introduces a random component in to the tree building
process that reduces the variance of a single tree\'s prediction and
improves predictive performance. However, the trees in bagging are not
completely independent of each other since all the original
predictorsare considered at every split of every tree. Rather, trees
from different bootstrap samples typically have similar structure to
each other due to underlying relationships.

### Literature Review

This section is going to cover the literature review...

## Methods

Steps to Build a Random Forest

-   Randomly select \"K\" features from total \"m\" features
    where k \< m

-   Among the \"K\" features, calculate the node \"d\" using the
    best split point

-   Split the node into daughter nodes using the best split
    method

-   Repeat the previous steps until you reach the \"l\" number
    of nodes

-   Build a forest by repeating all steps for \"n\" number
    times to create \"n\" number of trees

After the random forest trees and classifiers are created, predictions
can be made using the following steps:

-   Run the test data through the rules of each decision
    tree to predict the outcome and then store that predicted target
    outcome

-   Calculate the votes for each of the predicted targets

-   The most highly voted predicted target is the final
    prediction 

The basic algorithm for a regression random forest can be generalized to
the
following:

1.  Find data set

2.  Select number of trees to build (ntrees)

3.  for i = 1 to ntrees do

4.  Generate a bootstrap sample of the original data

5.  Grow a regression tree to the bootstrapped data

6.  For each split:

7.  Select m variables at random from all p variables

8.  Pick the best variable/split-point among the m

9.  Split the node into two child nodes

10\. End

11\. Use tree model stopping criteria to determine when a tree is
complete (but do not prune)

12\. End

\--

The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.

Another equation:

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data and Visualization

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

## Tuning

An approach which considers hyperparameters and involves tuning the
number of candidate variables to select from at each split. However,
there are a few additional hyperparameters that we should be aware of.
Although the argument names may differ across packages, these
hyperparameters should be present:

-   ntree: number of trees

    -   We want enough trees to stabilize the error.

-   mtry: number of variables to randomly sample as candidates at each
    split

    -   When mtry = *p*, this is similar to bagging.

    -   When mtry = 1, the split variable is random.

    -   A common suggestion is to start with 5 values evenly spaced
        across the range from 2 to *p*.

-   sampsize: number of samples

-   nodesize: minimum number of samples within the terminal nodes

    -   Controls complexity

-    maxnodes: maximum number of terminal nodes

    -    More nodes results in deeper, more complex trees, while fewer
        nodes produce shallower trees.

```{r}


```

### Conclusion

Conclusion here...

## References

Biau G, Scornet E. 2016. A random forest guided tour. TEST. 25:197--227.

Breiman, Leo. 2001. Random forests. Machine Learning. 45:5-32.

Segal MR. 2004. Machine learning benchmarks and random forest
regression. UCSF:
Center for Bioinformatics and Molecular Biostatistics*.*\[accessed 2024
Jan 16\]. [Machine Learning Benchmarks and Random Forest Regression
(escholarship.org)](https://escholarship.org/uc/item/35x3v9t4#main)

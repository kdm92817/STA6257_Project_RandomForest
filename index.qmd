---
title: "STA 6257 - Data Science Capstone"
author: "Group Random Forest"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is random forest?

Random forests are considered an "out of the box" machine learning
algorithm and are made up of a collection of decision trees to create a
"forest" [@breiman2001random]. This approach is useful in prediction and
regression analysis.

Random forests are constructed on the fundamental principles found
bagging, which introduces a random component in to the tree building
process that reduces the variance of a single tree's prediction and
improves predictive performance. However, the trees in bagging are not
completely independent of each other since all the original predictors
are considered at every split of every tree. Rather, trees from
different bootstrap samples typically have similar structure to each
other due to underlying relationships.

### Literature Review

This section is going to cover the literature review...

## Background

### **Decision trees** 

Decision trees start with a basic question, such as, \"Should I surf?\"
From there, you can ask a series of questions to determine an answer,
such as, \"Is it a long period swell?\" or \"Is the wind blowing
offshore?\". These questions make up the decision nodes in the tree,
acting as a means to split the data. Each question helps an individual
to arrive at a final decision, which would be denoted by the leaf node.
Observations that fit the criteria will follow the \"Yes\" branch and
those that don\'t will follow the alternate path.  Decision trees seek
to find the best split to subset the data, and they are typically
trained through the Classification and Regression Tree (CART) algorithm.
Metrics, such
as Gini impurity, information gain, or mean square error (MSE), can be
used to evaluate the quality of the split.  

This decision tree is an example of a classification problem, where the
class labels are "" and "."

-   Will rework and tailor example to dataset.

### **Applications** 

### **Limitations/Challenges**

While decision trees are common supervised learning algorithms, they can
be prone to problems, such as bias and overfitting. However, when
multiple decision trees form an ensemble in the random forest algorithm,
they predict more accurate results, particularly when the individual
trees are uncorrelated with each other.

-   Time-consuming process: Since random forest algorithms can handle
    large data sets, they can be provide more accurate predictions but
    can be slow to process data as they are computing data for each
    individual decision tree.

-   Requires more resources

-   More complex: The prediction of a single decision tree is
    easier to interpret when compared to a forest of them.

## Methods

### **Ensemble Method**

Ensemble learning methods are made up of a set of classifiers (i.e.,
decision trees), and their predictions are aggregated to identify the
most popular result. The most well-known ensemble methods are bagging,
also known as bootstrap aggregation, and boosting. The bagging method;
in this method, a random sample of data in a training set is selected
with replacement---meaning that the individual data
points can be chosen more than once. After several data samples are
generated, these models are then trained independently, and depending on
the type of task---i.e. regression or classification---the average or
majority of those predictions yield a more accurate estimate. This
approach is commonly used to reduce variance.

### **Random forest algorithm**

Steps to Build a Random Forest

-   Randomly select "K" features from total "m" features where k \< m

-   Among the "K" features, calculate the node "d" using the best split
    point

-   Split the node into daughter nodes using the best split method

-   Repeat the previous steps until you reach the "l" number of nodes

-   Build a forest by repeating all steps for "n" number times to create
    "n" number of trees

After the random forest trees and classifiers are created, predictions
can be made using the following steps:

-   Run the test data through the rules of each decision tree to predict
    the outcome and then store that predicted target outcome

-   Calculate the votes for each of the predicted targets

-   The most highly voted predicted target is the final prediction 

\--

The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.

Another equation:

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data and Visualization

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling 

### Tuning

An approach which considers hyperparameters and involves tuning the
number of candidate variables to select from at each split. However,
there are a few additional hyperparameters that we should be aware of.
Although the argument names may differ across packages, these
hyperparameters should be present:

-   ntree: number of trees

    -   We want enough trees to stabilize the error.

-   mtry: number of variables to randomly sample as candidates at each
    split

    -   When mtry = *p*, this is similar to bagging.

    -   When mtry = 1, the split variable is random.

    -   A common suggestion is to start with 5 values evenly spaced
        across the range from 2 to *p*.

-   sampsize: number of samples

-   nodesize: minimum number of samples within the terminal nodes

    -   Controls complexity

-   maxnodes: maximum number of terminal nodes

    -   More nodes results in deeper, more complex trees, while fewer
        nodes produce shallower trees.

```{r}


```

### Conclusion

Conclusion here...

## References

\@article{breiman2001random,

  title={Random forests},

  author={Breiman, Leo},

  journal={Machine learning},

  volume={45},

  pages={5\--32},

  year={2001},

  publisher={Springer}

}

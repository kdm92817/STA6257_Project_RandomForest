---
title: "STA 6257 - Data Science Capstone"
author: "Group Random Forest"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is random forest?

Random Forest is an ensemble learning method that combines multiple
decision trees to make predictions. The main idea behind Random Forest
is to introduce randomness in the training process by creating a diverse
set of decision trees. Each tree in the Random Forest is trained on a
random subset of the data and a random subset of features. By combining
the predictions of multiple trees, Random Forest reduces the risk of
overfitting and improves the overall accuracy and robustness of the
model. The randomness in the training process also helps in handling
high-dimensional data, handling missing values, and providing estimates
of variable importance. Random Forest has gained popularity due to its
ability to handle complex datasets and produce reliable predictions in
various domains.

## Literature Review

Biau (2012) delves into the statistical components and mathematical
support of the random forest model. Several theorems are outlined in
which to display the consistency of the model. Then, proof is provided
for those outcomes, with worked out equations that adjust for
inequalities and support the propositions. He touches in the importance
of variable selection and the increasing adaptability found within the
random forest algorithm.

Biau and Scornet (2016) provide an overview into the methodology,
practice, and recent developments of the random forest algorithm. Basic
principles are discussed, and mathematical support for the algorithm is
provided. They delve into the importance of variable selection and tree
parameters when performing predictive analysis.

Jaiswal & Samikannu (2017) detail the uses and capabilities of the
random forest model. They highlight the computation tools it employs to
overcome obstacles, such as missing data, outliers, noisy data, and/or
classification problems. The importance of the Gini index is described
as a measure of accuracy of the decision trees.

Liaw and Wiener (2002) discuss the introduction of the random forest
algorithm by L. Breiman, in addition to offering examples of its
application in the R interface. They discuss how the algorithm draws
bootstrap samples and estimates rates of error. It is noted that the
production of multiple trees is crucial in obtaining variable importance
and measures of proximity.

Lingjun et al. (2019) discuss uses and advantages of tree-based machine
learning algorithms. They highlight the key benefits of these models
over classical regression analyses, and they expound upon the predictive
capabilities as a strategy for data-based decision making. A simulation
experiment is outlined to allow for greater understanding of the process
and its corresponding results.

Segal (2004) offers a clear definition and practical application of
random forest regression. Two different profiles are established using
random forest methodology, and predictive errors are identified.
Mathematical equations lend support to the regression analysis. Segal
(2004) explains that one of guiding forces in random forest regression
is to increase variance by decreasing correlation.

## Background

### **Decision trees**

Decision trees start with a basic question, such as, "Should I surf?"
From there, you can ask a series of questions to determine an answer.
"Is it a long period swell?" or "Is the wind blowing offshore?".

These questions make up the decision nodes in the tree, acting as a
means to split the data. Each question aids in the arrival of a final
decision, which would be denoted by the leaf node.

Observations that fit the criteria will follow the "Yes" branch, and
those that do not will follow the alternate path.

Decision trees seek to find the best split to subset the data, and they
are typically trained through the Classification and Regression Tree
(CART) algorithm. Metrics, such as Gini impurity, information gain, or
mean square error (MSE), can be used to evaluate the quality of the
split.

This decision tree is an example of a classification problem, where the
class labels are "" and "."

-   Will rework and tailor example to dataset.

### **Applications**

#### **Benefits/Advantages**

-   Capable of performing both classification and regression tasks.
-   Capable of handling large datasets with high dimensionality.
-   Decreased training time compared to other algorithms.
-   Promotes predictive ability with high accuracy, even with large
    datasets.
-   Can solve problems internally and still maintain accuracy, even when
    a large proportion of data is missing.
-   Enhances the accuracy of the model and prevents issues in
    overfitting.
-   Can be used as a feature selection tool using its variable
    importance plot.

#### **Limitations/Challenges**

-   Prone to problems, such as bias and overfitting.
    -   However, when multiple decision trees form an ensemble in the
        random forest algorithm, they predict results with greater
        accuracy, particularly when the individual trees are
        uncorrelated with each other.
-   Time-consuming
    -   Since random forest algorithms can handle large data sets, they
        can be provide more accurate predictions. However, the process
        can be slow as they are computing data for each individual
        decision tree.
-   Requires more resources
-   More complex
    -   The prediction of a single decision tree is easier to interpret
        when compared to a forest of them.

## Methods

### **Ensemble Method**

Ensemble learning methods are made up of a set of classifiers (i.e.,
decision trees), and their predictions are aggregated to identify the
most popular result.

-   **Bagging**

    -   Also known as bootstrap aggregation
    -   A random sample of data in a training set is selected with
        replacement, meaning the individual data points can be chosen
        more than once.
    -   Models are trained independently.
    -   Final ouput is based on majority voting.
    -   This approach helps to reduce variance.

-   **Boosting**

    -   Combines weak learners into strong learners by creating
        sequential models such that the final model has the highest
        accuracy.

### **Random forest algorithm**

Steps to Build a Random Forest

-   Randomly select "K" features from total "m" features where k \< m
-   Among the "K" features, calculate the node "d" using the best split
    point
-   Split the node into daughter nodes using the best split method
-   Repeat the previous steps until you reach the "l" number of nodes
-   Build a forest by repeating all steps for "n" number times to create
    "n" number of trees

After the random forest trees and classifiers are created, predictions
can be made using the following steps:

-   Run the test data through the rules of each decision tree to predict
    the outcome and then store that predicted target outcome
-   Calculate the votes for each of the predicted targets
-   The most highly voted predicted target is the final prediction 

### **Assumptions**

-   Some of the actual values in the feature variable of the dataset
    should be present so the classifier can predict accurate results.

-   The predictions from each tree must have very low correlations.

## Analysis and Results

### Data and Visualization

#### Dataset

Data Source: Mendeley Data

\[Species data used in Random Forest modelling to determine predictors
of extinction\](https://data.mendeley.com/datasets/tc6syk8vwf/1)

**Description**

Data was extracted from the South African Red List database to compile a
profile for plant extinctions (SANBI 2021). South Africa offers a wide
array of biodiversity and estimates over 22,000 plant taxa. The
International Union for Conservation of Nature\'s (IUCN) Red List of
Threatened Species provides a standardized method to document and assess
extinctions (IUCN 2023). Species are classified into one of the
following groups: Extinct (EX), Extinct in the wild (EW), Critically
endangered possibly extinct (CR PE), Critically endangered (CR),
Endangered (EN), Vulnerable (VU), Near threatened (NT), Conservation
dependent (CD), Least concern (LC), and Data deficient (DD).

Plants are an essential component to an ecosystem\'s functionality, so
it is critical to evaluate drivers of extinction and determine methods
of prevention. To examine potential indicators for extinctions, extinct,
threatened, and non-threatened taxa are compared to identify and/or
distinguish traits that may be associated with risk or vulnerability.
The final dataset comprises 842 extant taxa, 33 Extinct taxa, and 69
Possibly Extinct (CR PE) taxa, to total 944 species
\[\@van2023species\].

**Table 1** organizes and summarizes the explanatory variables. 

The response variable is Extinction risk status grouped into Extinct,
Threatened (CR, EN, VU) and Not-Threatened (NT, LC, Critically Rare,
Rare).

**Table 1.**

![](images/image-613595486.png)\

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data


```

### Statistical Modeling

#### Tuning

An approach which considers hyperparameters and involves tuning the
number of candidate variables to select from at each split. However,
there are a few additional hyperparameters that we should be aware of.
Although the argument names may differ across packages, these
hyperparameters should be present:

-   ntree: number of trees
    -   We want enough trees to stabilize the error.
-   mtry: number of variables to randomly sample as candidates at each
    split
    -   When mtry = *p*, this is similar to bagging.
    -   When mtry = 1, the split variable is random.
    -   A common suggestion is to start with 5 values evenly spaced
        across the range from 2 to *p*.
-   sampsize: number of samples
-   nodesize: minimum number of samples within the terminal nodes
    -   Controls complexity
-   maxnodes: maximum number of terminal nodes
    -   More nodes results in deeper, more complex trees, while fewer
        nodes produce shallower trees.

```{r}


```

### Conclusion

Conclusion here...

## References

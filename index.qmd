---
title: "STA 6257 - Data Science Capstone"
author: "Group Random Forest"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is random forest?

Random forests are considered an "out of the box" machine learning
algorithm and are made up of a collection of decision trees to create a
"forest". This approach aims to improve prediction accuracy and reduce
risk of overfitting by combining multiple decision trees.

In a Random Forest model, a set of decision trees are created using a
random subset of the training data and a random set of features at each
split. Each tree in the forest is trained independently, and the final
prediction is made by aggregating the predictions of all the trees in
the forest. The aggregation method varies depending on the task at hand.

For classification, the finalprediction is typically made by taking the
majority vote of the individual tree predictions, while in regression,
the final prediction is made by taking the mean of the individual tree
predictions.

### Literature Review

Biau and Scornet (2016) provide an overview into the methodology,
practice, and recent developments of the random forest algorithm. Basic
principles are discussed, and mathematical support for the algorithm is
provided. They delve into the importance of variable selection and tree
parameters when performing predictive analysis.

Segal (2004) offers a clear definition and practical application of
random forest regression. Two different profiles are established using
random forest methodology, and predictive errors are identified.
Mathematical equations lend support to the regression analysis. Segal
(2004) explains that one of guiding forces in random forest regression
is to increase variance by decreasing correlation.

Lingjun et al. (2019) discuss uses and advantages of tree-based machine
learning algorithms. They highlight the key benefits of these models
over classical regression analyses, and they expound upon the predictive
capabilities as a strategy for data-based decision making. A simulation
experiment is outlined to allow for greater understanding of the process
and its corresponding results.

Jaiswal & Samikannu (2017) details the uses and capabilities of the
random forest model. They highlight the computation tools it employs to
overcome obstacles, such as missing data, outliers, noisy data, and/or
classification problems. The importance of the Gini index is described
as a measure of accuracy of the decision trees.

Biau (2012) delves into the statistical components and mathematical
support of the random forest model. Several theorems are outlined in
which to display the consistency of the model. Then, proof is provided
for those outcomes, with worked out equations that adjust for
inequalities and support the propositions. He touches in the importance
of variable selection and the increasing adaptability found within the
random forest algorithm.

Liaw and Wiener (2002) discuss the introduction of the random forest
algorithm by L. Breiman, in addition to offering examples of its
application in the R interface. They discuss how the algorithm draws
bootstrap samples and estimates rates of error. It is noted that the
production of multiple trees is crucial in obtaining variable importance
and measures of proximity.

## Background

### **Decision trees**

Decision trees start with a basic question, such as, "Should I surf?"
From there, you can ask a series of questions to determine an answer.
"Is it a long period swell?" or "Is the wind blowing offshore?".

These questions make up the decision nodes in the tree, acting as a
means to split the data. Each question aids in the arrival of a final
decision, which would be denoted by the leaf node. Observations that fit
the criteria will follow the "Yes" branch, and those that do not will
follow the alternate path.

Decision trees seek to find the best split to subset the data, and they
are typically trained through the Classification and Regression Tree
(CART) algorithm. Metrics, such as Gini impurity, information gain, or
mean square error (MSE), can be used to evaluate the quality of the
split.

This decision tree is an example of a classification problem, where the
class labels are "" and "."

-   Will rework and tailor example to dataset.

### **Applications**

### **Limitations/Challenges**

While decision trees are common supervised learning algorithms, they can
be prone to problems, such as bias and overfitting. However, when
multiple decision trees form an ensemble in the random forest algorithm,
they predict more accurate results, particularly when the individual
trees are uncorrelated with each other.

-   Time-consuming process: Since random forest algorithms can handle
    large data sets, they can be provide more accurate predictions.
    However, the process can be slow as they are computing data for each
    individual decision tree.
-   Requires more resources
-   More complex: The prediction of a single decision tree is easier to
    interpret when compared to a forest of them.

## Methods

### **Ensemble Method**

Ensemble learning methods are made up of a set of classifiers (i.e.,
decision trees), and their predictions are aggregated to identify the
most popular result. The most well-known ensemble methods are bagging,
also known as bootstrap aggregation, and boosting. In the bagging
method, a random sample of data in a training set is selected with
replacement, meaning the individual data points can be chosen more than
once. After several data samples are generated, these models are then
trained independently, and depending on the type of task---i.e.,
regression or classification---the average or majority of those
predictions yield a more accurate estimate. This approach is commonly
used to reduce variance.

### **Random forest algorithm**

![](images/image-2130503952.png)

Steps to Build a Random Forest

-   Randomly select "K" features from total "m" features where k \< m
-   Among the "K" features, calculate the node "d" using the best split
    point
-   Split the node into daughter nodes using the best split method
-   Repeat the previous steps until you reach the "l" number of nodes
-   Build a forest by repeating all steps for "n" number times to create
    "n" number of trees

After the random forest trees and classifiers are created, predictions
can be made using the following steps:

-   Run the test data through the rules of each decision tree to predict
    the outcome and then store that predicted target outcome
-   Calculate the votes for each of the predicted targets
-   The most highly voted predicted target is the final predictionÂ 

\--

The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.

Another equation:

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data and Visualization

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Statistical Modeling

### Tuning

An approach which considers hyperparameters and involves tuning the
number of candidate variables to select from at each split. However,
there are a few additional hyperparameters that we should be aware of.
Although the argument names may differ across packages, these
hyperparameters should be present:

-   ntree: number of trees
    -   We want enough trees to stabilize the error.
-   mtry: number of variables to randomly sample as candidates at each
    split
    -   When mtry = *p*, this is similar to bagging.
    -   When mtry = 1, the split variable is random.
    -   A common suggestion is to start with 5 values evenly spaced
        across the range from 2 to *p*.
-   sampsize: number of samples
-   nodesize: minimum number of samples within the terminal nodes
    -   Controls complexity
-   maxnodes: maximum number of terminal nodes
    -   More nodes results in deeper, more complex trees, while fewer
        nodes produce shallower trees.

```{r}


```

### Conclusion

Conclusion here...

## References

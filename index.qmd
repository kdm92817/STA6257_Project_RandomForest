---
title: "Predicting Plant Exctinctions in South Africa: A Random Forest Approach"
author: "Kristen Monaco, Raymond Fleming, Teng Ma"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is random forest?

Random Forest is an ensemble learning method that combines multiple
decision trees to make predictions. The main idea behind Random Forest
is to introduce randomness in the training process by creating a diverse
set of decision trees. Each tree in the Random Forest is trained on a
random subset of the data and a random subset of features. By combining
the predictions of multiple trees, Random Forest reduces the risk of
overfitting and improves the overall accuracy and robustness of the
model. The randomness in the training process also helps in handling
high-dimensional data, handling missing values, and providing estimates
of variable importance. Random Forest has gained popularity due to its
ability to handle complex datasets and produce reliable predictions in
various domains.

[Quarto Slides](slides.html)

## Literature Review

In the paper "Analysis of a Random Forests Model", Biau
[@biau2012analysis] provides an in-depth analysis of the Random Forests
model. The author delves into the statistical components and
mathematical support, as well as investigates various aspects of the
Random Forests, such as its consistency, convergence rates, and the
effect of the number of trees on performance. He touches on the
importance of variable selection and the increasing adaptability found
within the random forest algorithm. Biau provides theoretical insights
into the behavior of Random Forests, shedding light on its robustness
and effectiveness as a machine learning algorithm.

In "A Random Forest Guided Tour", Biau and Scornet
[@biau2016random]present an extensive exploration of Random Forests, a
popular machine learning algorithm. The paper serves as a comprehensive
guide, covering aspects of theoretical foundations, methodology, model
evaluation, and empirical performance. They discuss the inner workings
of Random Forests, including topics such as tree construction, feature
selection, and ensemble learning, which aid in predictive analysis.
Overall, the paper serves as a valuable resource for those interested in
understanding and utilizing Random Forests in their machine learning
tasks.

In "Classification and Regression by randomForest," Liaw and Wiener
[@liaw2002classification] discuss the introduction of the randomForest
package for classification and regression tasks in the R programming
language. An overview of the Random Forest algorithm is provided,
including how it draws bootstrap samples and estimates rates of error.
This technique is referred to as ensemble learning called bagging and
uses decision trees as base learners. Implementation of the randomForest
package addresses parameters for tuning the model and handling missing
values. It is noted that the production of multiple trees is crucial in
obtaining variable importance and measures of proximity. Overall, the
paper serves as a practical guide for R users and highlights the
advantages of using random forests for both classification and
regression problems, such as robustness to overfitting and high
prediction accuracy.

In the paper "Random Forest as a Predictive Analytics Alternative to
Regression in Institutional Research", the authors [@lingjun2019random]
explore the application of Random Forests as a predictive analytics tool
in institutional research. They discuss the limitations of traditional
regression models in handling complex datasets and introduce Random
Forest as an alternative approach. To do this, they highlight the key
benefits of the Random Forest model, as it has the ability to handle
non-linear relationships, interactions, and high-dimensional data
effectively. In addition, they expound upon predictive capabilities,
improved accuracy, and robustness as a strategy for data-based decision
making.

In "Machine Learning Benchmarks and Random Forest Regression", Segal
[@segal2004machine] explores the application and effectiveness of Random
Forest Regression in the context of machine learning. Machine learning
algorithms may face challenges, which emphasize the need for
standardized datasets and evaluation metrics. Segal (2004) examines how
Random Forest Regression performs across various datasets, comparing its
performance to other regression techniques. The study highlights the
strengths of Random Forest Regression in handling non-linear
relationships and noisy data, showcasing its versatility and robustness.
Additionally, the paper provides insights into the impact of different
parameters on the performance of Random Forest Regression, offering
practical guidance for its implementation. Overall, the paper
contributes to the understanding of Random Forest Regression and its
potential as a powerful tool in the field of machine learning.

## Methods

### Decision Trees

Decision Trees are a type of Supervised Machine Learning where the data
is continuously split according to a certain parameter. Decision trees
serve as the fundamental building blocks of the Random Forest algorithm.
They are a powerful and intuitive tool used for both classification and
regression tasks. Decision trees represent a flowchart-like structure,
where each internal node represents a decision based on a feature, and
each leaf node represents a final decision or prediction. They ask a
series of questions, such as those with a yes or no response or will a
coin flip come up heads or tails. They do this to split the data and
minimize the variance for the target variable.

Decision trees in random forest work together to create a robust and
accurate model by leveraging the diversity and averaging multiple
decision trees.

![](images/image-1963384308.png)

When evaluating the quality of the splits in decision trees, several
metrics are considered.

**1. Gini impurity:**

The Gini coefficient is a measure of inequality between 0 and 1, where a
value closer to 1 indicates more inequality. This is used to calculate
inequality in income and wealth, but it can be used in other problems
where a measure of inequality is needed. $$
G = \frac{\sum_{i=1}^{n}\sum_{j=1}^{n}|x_i - x_j|}{2n^2\bar{x}}
$$

**2. Information gain:**

This measure can be used in multiclass problems and is often used for
finding where to split the features. The other commonly used option is
Entropy, which is also an entropy measure but with a more complex
calculation.

The tradeoff is a faster calculation using the Gini impurity, with a
possibly higher accuracy. $$H=\sum\limits_{i=1}^{n}-p_i log_2 p_i$$
where $p_i$ is the percentage of each class present in the node
resulting from a tree split.

**3. Entropy:**

For decision trees, a related measure is used to measure the entropy of
a result by calculating the probability that a random entry will be in
the wrong class if it were randomly assigned a label.
$$I_G=1-\sum\limits_{i=1}^{J}p_i^2$$ where $p_i$ is the probability that
a randomly labeled element would be incorrectly classified.

**4. Out of bag error estimation:**

-   Each decision tree is trained on a bootstrapped sample of the
    original dataset.
-   The data points that are not included in the bootstrapped sample for
    a particular tree are called out-of-bag instances.
-   OOB error provides a measure of how well the random forest model is
    likely to perform on new data.
-   It is useful for assessing model performance and tuning
    hyperparameters.

$$\text{OOB Error} = \frac{1}{N} \sum_{i=1}^{N} I(y_i \neq \hat{y}_{i, \text{OOB}})
$$ where $N$ is the number of rows in the dataset and $I$ is an
indicator function which returns 1 if $y_i \ne \hat{y}_{i,OOB}$

### Random Forest involves the following basic concepts:

1.  **Bootstrap Sampling (Bagging):** Perform random sampling with
    replacement on the original dataset to form a new dataset for
    training a decision tree. In each round of bootstrap sampling, about
    36.8% of the samples will be missed, not appearing in the new
    dataset, and these data are referred to as out-of-bag (OOB) data.

2.  **Random Feature Selection:** At each node of the decision tree
    training, randomly select a subset of features, and then use
    information gain (or other criteria) to choose the best split.
    Repeat the above steps, generating multiple decision trees, to form
    a "forest".

3.  **Prediction:** When predicting new data samples, each tree produces
    its own prediction result. Random Forest combines these results and
    uses majority voting to determine the final prediction outcome.

4.  **Ensemble Learning via Hard Voting Classifier:** By voting, the
    results of five models are integrated to get the combined outcome.
    The result selects the category that appears most frequently as the
    final prediction result. This is a strategy of ensemble learning,
    known as the Hard Voting Classifier.

### Five normalization methods

The data needs to be normalized using 5 different methods, due to
significant differences in feature values initially:

-   **Min-Max Normalization**

    $X_{new}=\frac{X_{old}-\min(X_{old})}{\max(X_{old})-\min(x_{old})}$

-   **Z-Score Normalization**

    $X_{new}=\frac{X_{old}-\bar{X}_{old}}{\sigma_{X_{old}}}$

-   **Max Absolute Scaling**

    $X_{new}=\frac{X_{old}}{\max(|X_{old}|)}$

-   **L1 Norm Normalization**

    $X_{new}=\frac{X_{old}}{\sum(|X_{old}|)}$

-   **L2 Norm Normalization**

    $X_{new}=\frac{X_{old}}{\sqrt{\sum{X_{old}^2}}}$

### Evaluation

Evaluate the test set and calculate four metrics (accuracy, recall,
precision, F1).

::: columns
::: {.column width="12%"}
$\text{Accuracy}$\
\
:::

::: {.column width="35%"}
$=\frac{\sum{\left(\text{Actual Label} = \text{Predicted Label}\right)}}{\text{Label Count}}$\
\
:::

::: {.column width="53%"}
:::
:::

::: columns
::: {.column width="12%"}
$\text{Recall}$\
\
:::

::: {.column width="35%"}
$=\frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}$\
\
:::

::: {.column width="53%"}
:::
:::

::: columns
::: {.column width="12%"}
$\text{Precision}$\
\
:::

::: {.column width="35%"}
$=\frac{\text{True Positives}}{\text{True Positives}+\text{False Positives}}$\
\
:::

::: {.column width="53%"}
:::
:::

::: columns
::: {.column width="12%"}
$\text{F1}$
:::

::: {.column width="35%"}
$=\frac{2*(\text{Precision}*\text{Recall})}{\text{Precision}+\text{Recall}}$\
:::

::: {.column width="53%"}
:::
:::

### Final outcome:

The idea is to combine five models using a voting method and pick the
category with the most votes as our final prediction. It's like a team
effort in machine learning, known as the Hard Voting Classifier. For a
sample point x, five models make predictions respectively:

$y_1 = \text{MinMax}(x)$\
\
$y2 = \text{ZScore}(x)$\
\
$y3 = \text{MaxAbsoluteValue}(x)$\
\
$y4 = \text{L1Norm}(x)$\
\
$y5 = \text{L2Norm}(x)$\
\
Place these five prediction results into a set
$Y = \{y_1,y_2,y_3,y_4,y_5\}$. The final prediction result $y_{final}$
is the element that appears most frequently in the set $Y$,
mathematically represented as $y_{final} = \text{mode}(Y)$.

## Assumptions

-   Independence of trees: Each tree is built using a random subset of
    features and a bootstrapped sample of the data, aiming to reduce
    correlation between trees.
-   Randomness: While some of the actual values in the feature variable
    of the dataset should be present so the classifier can predict
    accurate results, random forests must also assume a random subset of
    features to reduce the probability of overfitting and encourage tree
    diversity.

# Analysis and Results

## Data and Visualization

Data Source: **Mendeley Data**

[Species data used in Random Forest modelling to determine predictors of
extinction](https://data.mendeley.com/datasets/tc6syk8vwf/1)

**Description** Data was extracted from the South African Red List
database [@SANBI2021] to compile a profile for plant extinctions. South
Africa offers a wide array of biodiversity and estimates over 22,000
plant taxa. The International Union for Conservation of Nature's (IUCN)
[@IUCN2023] Red List of Threatened Species provides a standardized
method to document and assess extinctions (IUCN 2023). Species are
classified into one of the following statuses: Extinct (EX), Extinct in
the wild (EW), Critically endangered possibly extinct (CR PE),
Critically endangered (CR), Endangered (EN), Vulnerable (VU), Near
threatened (NT), Conservation dependent (CD), Least concern (LC), and
Data deficient (DD).

Plants are an essential component to an ecosystem's functionality, so it
is critical to evaluate drivers of extinction and determine methods of
prevention. To examine potential indicators, extinct, threatened, and
non-threatened taxa are compared to identify and/or distinguish traits
that may be associated with risk or vulnerability. The final dataset
comprises 842 extant taxa, 33 Extinct taxa, and 69 Possibly Extinct (CR
PE) taxa, to total 944 species.

The table below organizes and summarizes the explanatory variables.

| **Type of Variable** | **Variable Name**   | **Description**                                                                                                                                                                                                                                                         | **Range of Values** |
|-----------------|-----------------|---------------------|-----------------|
| **Binary**           | Habitat loss        | Conversion, fragmentation, and/or elimination of habitat. e.g., logging, wood harvesting, livestock farming, urban development.                                                                                                                                         | (0, 1)              |
|                      | Habitat degradation | Alteration of natural habitats necessary for species survival resulting in reduced functionality e.g., fire suppression, droughts.                                                                                                                                      | (0, 1)              |
|                      | Invasive species    | Impacts of alien species on natives through different mechanisms e.g. alteration of soil chemistry, resource competition.                                                                                                                                               | (0, 1)              |
|                      | Pollution           | Pollutants entering the natural environment e.g., air-borne pollutants, waste.                                                                                                                                                                                          | (0, 1)              |
|                      | Over-exploitation   | Excessive use of species causing decreases in viable populations e.g. overharvesting.                                                                                                                                                                                   | (0, 1)              |
|                      | Other               | Intrinsic factors, changes in native taxa dynamics, human disturbance, natural disasters.                                                                                                                                                                               | (0, 1)              |
|                      | Unknown             | N/A                                                                                                                                                                                                                                                                     | (0, 1)              |
| **Categorical**      | Life form (LF)      | Annual or perennial                                                                                                                                                                                                                                                     | (0, 1)              |
|                      | Growth form (GF)    | One of 14 distinct forms: Parasitic plant, Tree, Shrub, Suffrutex, Herb, Lithophyte, Succulent, Graminoid, Geophyte, Climber, Carnivorous, Cyperoid, Creeper, Epiphyte.                                                                                                 | (1,14)              |
|                      | Biomes              | One of nine biomes present in South Africa: Fynbos, Grassland, Succulent Karoo, Albany Thicket, Savanna, Forest, Nama Karoo, Desert, Indian Ocean Coastal Belt. \*Note: if a taxon was found in multiple biomes it was marked as generalist.                            | (1,10)              |
| **Continuous**       | Range size          | All species range sizes are based on the standard measure of Extent of Occurrence (EOO), a parameter defined as the shortest continuous imaginary boundary that can be drawn to encompass all the known, inferred, or projected sites of present occurrence of a taxon. | (1,1855022)         |
| **Descriptive**      | Family              | Taxonomic categorization                                                                                                                                                                                                                                                | (1, 104)            |
|                      | Status              | Current Red List Category Designation                                                                                                                                                                                                                                   | (1, 11)             |
| **Target**           | Group               | Threatened, Not Threatened, or Extinct                                                                                                                                                                                                                                  | (1,3)               |

#### The preview of the dataset

```{r}
library(caret)
library(caTools)
library(corrplot)
library(DescTools)
library(dplyr)
library(dslabs)
library(fastDummies)
library(ggcorrplot)
library(ggthemes)
library(ggplot2)
library(gridExtra)
library(gsheet)
library(gtsummary)
library(ipred)
library(knitr)
library(lessR)
library(magrittr)
library(plyr)
library(randomForest)
library(readr)
library(ROSE)
library(rpart)       
library(rpart.plot)  
library(rsample)     
library(splitTools)
library(tidyverse)
library(vip)
library(visreg)


data <- as_tibble(gsheet2tbl("https://docs.google.com/spreadsheets/d/1BJui4r7xoVY6e3z2-IgCr_eBn7SvNAQ4BEpPo1GILNU/edit?usp=sharing"))

ggplot(data, aes(x = factor(Status), fill = factor(Status))) + 
  geom_bar(show.legend = FALSE) +
  scale_fill_brewer(palette = "Paired") +
  labs(title = "Distribution of Species by Conservation Status", 
       x = "Status", 
       y = "Frequency") +
  theme_minimal() +
  theme(text = element_text(size = 12),
        plot.title = element_text(hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid.major = element_line(color = "grey80"),
        panel.grid.minor = element_blank())

```

The bar plot provides an overview of frequency for each species by
conservation status. This enables a concise visualization of categorical
data, with each bar directly corresponding to the magnitude of its
respective category.

The plot clearly displays imbalance in the dataset, with the 'LC'
category drastically outnumbering the others with a frequency that
surpasses 500. To effectively tackle this issue and reduce potential
bias, it might be necessary to use techniques for resampling the data
that can boost the representation of underrepresented classes or balance
out the dominance of the overrepresented class. By implementing these
strategies, any insights or models derived from this analysis reflect a
more balanced dataset, avoiding any bias caused by imbalanced data. In
order to achieve this goal, the ROSE package has been chosen. This is a
tool specifically designed for addressing imbalances in datasets. With
its advanced methods for generating synthetic data and resampling, ROSE
enables us to achieve a fairer distribution of classes and thereby
enhance the reliability and validity of our analytical outcomes.

#### Feature Associations Plot

```{r}

train<-data
corrDF <- train %>% mutate(Range=ntile(Range, n=20))
corrDF <- corrDF %>% select(!Group) %>% mutate(across(c("Group","LF","GF","Range",
                                     "Biomes","Range","Habitat_degradation",
                                     "Habitat_loss","IAS","Other",
                                     "Over_exploitation","Pollution","Unknown"),
                                     as_factor))

ggcorrplot::ggcorrplot(DescTools::PairApply(corrDF,DescTools::CramerV), type='lower')
```

The image is a ggcorrplot, which is a way of visualizing how different
ecological factors like "Pollution" and "Habitat_loss" are associated to
each other. The colors in the plot show how strong these relationships
are, with red indicating positive correlation. The diagonal shows that
when variables are compared to themselves, there is a perfect positive
association. The squares in this heatmap show how the variables on the x
and y axes are related. A correlation of 1 means they have a perfect
positive relationship, -1 means they have a perfect negative
relationship, and 0 means there's no relationship at all. The colors
represent the strength and direction of the correlations: red shades
mean there's a positive correlation, blue shades mean there's a negative
correlation, and the intensity of the color shows how strong the
correlation is. The variables used are related to ecology and biology,
like "Pollution", "Over_exploitation", "Habitat_loss", and "Biomes".

#### Distribution of Range by Conservation Status

```{r}
ggplot(data = data, aes(x = Status, y = Range, fill = Status)) +
  geom_boxplot() +
  theme_bw() +
  ylim(0,100000)
```

This boxplot shows the sizes of species' habitats based on their
conservation status. It tells us how large or small the habitats are for
each category. On the vertical axis is the habitat size in square
kilometers, while the horizontal axis represents the conservation
status. We can intuit that as the magnititude of a species' conservation
status increases, their associated range becomes more restricted.

#### Distribution of Binary Variables with a Yes/No Response

```{r}
data2 <- as_tibble(gsheet2tbl("https://docs.google.com/spreadsheets/d/1BJui4r7xoVY6e3z2-IgCr_eBn7SvNAQ4BEpPo1GILNU/edit?usp=sharing"))

Value <- c(data2$Habitat_degradation, data2$Habitat_loss, data2$IAS,
              data2$Other, data2$Over_exploitation, data2$Pollution,
              data2$Unknown)
ExpVariable <- rep(c("Degradation", "Loss", "IAS",
                  "Other", "Exploitation", "Pollution",
                  "Unknown"), each= 944)
DataGroup <- data2$Group

DV <- data.frame(DataGroup, ExpVariable, Value)

ggplot(DV,aes(x=ExpVariable,y=Value, fill=Value)) +
  geom_bar(stat = "identity") +
  scale_fill_brewer() +
  labs(x= "Variable",
       y= "Frequency",
       ) +
  theme_bw()

```

This bar chart shows how often different environmental factors are
reported, with the vertical axis representing frequency and the
horizontal axis listing the factors: Degradation, Exploitation, IAS
(Invasive Alien Species), Loss, Other, Pollution, and Unknown. The
colors indicate whether each factor is present (darker shade for Yes) or
absent (lighter shade for No). This visualization allows a quick
interpretation into which factors are most commonly reported.

# Statistical Modeling

## Packages

The R packages utilized for running the statistical modeling for the
Random Forest algorithm were **readr**, **plyr**, **ipred**, **caret**,
**caTools**, **randomForest**, **ROSE**. - readr: Part of the tidyverse,
readr is designed for reading rectangular data, particularly CSVs
(comma-separated values) and other delimited types of text files. It's
known for its speed and for providing more informative error messages
compared to base R functions like read.csv. It also converts data into
tibbles, which are a modern take on data frames.

-   **plyr**: This package is used for splitting, applying, and
    combining data. plyr is known for its capability to handle different
    data types (arrays, lists, data frames, etc.) and apply functions to
    each element of the split data, then combine the results. Note that
    plyr is largely superseded by dplyr (also part of tidyverse), which
    is more efficient especially for large datasets.

-   **ipred**: Standing for "Improved Predictors", ipred provides
    functions for predictive modeling. It includes methods for bagging
    (Bootstrap Aggregating), which helps improve the stability and
    accuracy of machine learning algorithms, particularly for decision
    trees.

-   **caret**: The caret package (short for Classification And
    REgression Training) is a comprehensive framework for building
    machine learning models in R. It simplifies the process of model
    training, tuning, and predicting by providing a unified interface
    for various machine learning algorithms.

-   **caTools**: This package contains several tools for handling data,
    including functions for reading/writing Binary Large Objects
    (BLOBs), moving window statistics, and splitting data into
    training/testing sets. It's often used for its simple and effective
    method for creating reproducible train/test splits.

-   **randomForest**: As the name suggests, this package is used for
    implementing the Random Forest algorithm for classification and
    regression tasks. Random Forest is an ensemble learning method that
    operates by constructing a multitude of decision trees and
    outputting the mode of the classes (classification) or mean
    prediction (regression) of the individual trees.

-   **ROSE**: Standing for Random OverSampling Examples, the ROSE
    package is used to deal with imbalanced datasets in binary
    classification problems. It generates synthetic samples in a
    two-class problem to balance the class distribution, using smoothed
    bootstrapping. This helps improve the performance of classification
    models on imbalanced datasets.

## Data Preparation

Encode the 15 columns, with 'group' categorized into 1, 2, and 3. 'Yes'
should be encoded as 1, 'No' as 0, and other categories should be
numbered starting from 1. In the first column, replace the "-" with a
".".

### Original Dataset Preview

```{r}
kable(head(data, 10))
```

### Encoding Dataset Preview

```{r, warning=FALSE, echo=TRUE}
EncDF <- data
EncDF <- data %>% select(LF,GF,Biomes,Status,Range,
                Habitat_degradation,Habitat_loss,IAS,Other,
                  Over_exploitation,Pollution,Unknown,Group)
EncDF <- EncDF %>% 
            mutate(across(c(
              "Group","LF","GF","Biomes","Status","Range",
              "Habitat_degradation","Habitat_loss","IAS",
              "Other","Over_exploitation","Pollution",
              "Unknown"),as_factor))
EncDF <- EncDF %>% 
            mutate_at(c("Group","LF","GF","Biomes",
           "Status","Range","Habitat_degradation",
           "Habitat_loss","IAS","Other",
           "Over_exploitation","Pollution","Unknown"),
           as.numeric)
EncDF <- EncDF %>% mutate(across(c("LF","IAS","Other","Over_exploitation",                        "Pollution","Unknown"),function (x) x-1))


process <- as_tibble(gsheet2tbl("https://docs.google.com/spreadsheets/d/1MWhUji9lXGUf2boK3ALt31Fb6pwIluA5fFj-zT0yKbo/edit#gid=1072161265"))
kable(head(process, 10))
```

Split the dataset into a training set and a test set with a ratio of
7:3. Perform class imbalance handling on the training set using the ROSE
library, aiming for an equal data quantity among classes, represented as
1:1:1.

Given a minority class sample point $x$, we find its $k$ nearest
neighbors. Then, one neighbor is randomly selected, denoted as $z$, and
a new data point $y$ is constructed which lies on the line segment
between $x$ and $z$:

$y = x + \lambda * (z - x)$

Here, $\lambda$ is a random number between 0 and 1.

### Data Processing

1.  Process the data by setting the first 14 columns as \[features\] and
    the last column as the \[label\]
2.  Split the dataset into training and testing sets
3.  Combine the training datasets
4.  Print the initial number of each category

```{r}

data <- process
features <- data[, 1:14]
label <- data[, 15]

set.seed(42)

split <- sample.split(label, SplitRatio = 0.7)
features_train = features[split,]
features_test = features[!split,]
label_train = label[split]
label_test = label[!split]

data_train <- features_train
data_train$label <- label_train
class_counts <- table(data_train$label)

class_counts <- table(data_train$label)

```

#### Class Balancing

1.  Process classes A and B
2.  Process classes A and C
3.  Retain records in data_train_AB_resampled where the label is '2'
4.  Retain records in data_train_AC_resampled where the label is '3'
5.  Retain records in both data_train_AB_resampled and
    data_train_AC_resampled where the label is '1'
6.  Combine
7.  Print the number of each category after class imbalance handling

Group Counts Pre-Balancing: 490 148  23

Group Counts Post-Balancing: 490 490 490

#### Normalization

1.  Divide the features and label, and apply different normalization to
    the training and testing sets
2.  Apply Min-Max normalization to features_train and features_test
3.  Apply Z-Score normalization to each column of features_train
4.  Apply Max Absolute Value normalization to the training set
5.  Apply L1 norm normalization to the training set
6.  Apply L2 norm normalization to the training set

#### Training dataset

1.  Training dataset after Min-Max normalization
2.  Training dataset after Z-Score normalization
3.  Training dataset after Max Absolute Value normalization
4.  Training dataset after L1 norm normalization
5.  Training dataset after L2 norm normalization

#### Training Steps for Each Model

1.  Calculate and print the accuracy of the test set
2.  Convert to a categorical variable
3.  Obtain the confusion matrix
4.  Calculate the average recall rate (Sensitivity)
5.  Calculate the average precision rate (Positive Predictive Value)
6.  Print results

#### MinMax Model

#### ZScore Model

#### Max Absolute Value Model

#### L1 Normalization Model

#### L2 Normalization Model

#### Prediction

1.  Obtain the number of predicted results
2.  Initialize an empty vector to store the final prediction results
3.  Iterate over each test sample
4.  Get the prediction results of the five models for the i-th sample
5.  Select the most frequently predicted class as the final prediction
    result for the i-th sample
6.  Now final_pred contains the prediction results after voting
7.  Calculate and print the accuracy
8.  Convert to a factor type
9.  Obtain the confusion matrix
10. Calculate the recall (Sensitivity) for each category
11. Calculate the precision for each category



#### Confusion Matrix


## Conclusion

Random Forest is a powerful and flexible machine learning algorithm that
can be used for a wide range of tasks. It is particularly useful when
dealing with complex data composed of a large number of features and
when the goal is to achieve high predictive accuracy while avoiding
overfitting. The algorithm incorporates versatility in its capabilities
for classification and regression tasks, handling missing data, and
displaying robustness when faced with outliers and noisy data.

We produced a predictive model with 88% accuracy, indicating that our
explanatory variables were able to differentiate between non threatened,
threatened, and extinct taxa. Extinct species were classified with 100%
specificity and 70% sensitivity. Most extinctions were perennial shrubs
found in the Cape Floristic Region, a global biodiversity hotspot. As
range was the strongest predictor of extinction, many of the recorded
taxa deemed susceptible were range-restricted. Habitat loss is presented
as the second strongest variable of importance in predicting plant
extinctions. Predictions were based on a quantitative, evidence-based
approach, though gaps in knowledge highlighted areas for further study.
Improved species monitoring and documentation of threat factors will aid
in a deeper understanding of the ecological role and value of South
African plant species.

## References
